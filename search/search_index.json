{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RBA all day","text":""},{"location":"#welcome-to-the-wonderful-world-of-risk-based-alerting","title":"Welcome to the wonderful world of Risk-Based Alerting!","text":"<p>RBA is Splunk's method to aggregate low-fidelity security events as interesting observations tagged with security metadata to create high-fidelity, low-volume alerts.</p>"},{"location":"#searches","title":"Searches","text":"<p>Useful SPL from the RBA community for working with risk events.</p>"},{"location":"#dashboards","title":"Dashboards","text":"<p>Simple XML or JSON for Splunk dashboards to streamline risk analysis.</p>"},{"location":"#risk-rules","title":"Risk Rules","text":"<p>Splunk's Threat Research Team has an incredible library of over 1000 detections in the Splunk's Enterprise Security Content Updates library. You can use Marcus Ferrera and Drew Church's awesome ATT&amp;CK Detections Collector to pop out a handy HTML file of relevant ESCU detections for you to align with MITRE ATT&amp;CK.</p>"},{"location":"#the-rba-community","title":"The RBA Community","text":"The RBA Community<pre><code>    Join the RBA Community Today!\n</code></pre> <p>The RBA Community is a group of professionals dedicated to advancing the field of risk-based alerting (RBA) and Splunk Enterprise Security (ES). Our mission is to provide a forum for sharing knowledge, best practices, and the latest developments in RBA and ES, and to help professionals enhance their understanding and skills in these areas.</p> <p>Whether you\u2019re new to RBA and ES or a seasoned pro, The RBA Community has something for everyone. We invite you to join us on this journey to enhance your understanding and expertise in RBA and ES \u2013 don\u2019t miss out on this opportunity to learn from the best and connect with other professionals in the field.</p> <p>Learn more </p>"},{"location":"#contributing","title":"Contributing","text":"<p>Want to contribute? See our contributing guidelines.</p>"},{"location":"#discussionfaq","title":"Discussion/FAQ","text":"<p>See discussions and frequently asked questions on our GitHub Discussions board.</p> <p>Visit Discussion Board </p>"},{"location":"contributing/contributing-guidelines/","title":"Contributing Guidelines","text":"<p>All are welcome to contribute!</p> <p>A GitHub account is required to create a pull request to submit new content. If you do not want to submit changes, you may also consider the following:</p> <ul> <li>Submit a feature request (issue) at https://github.com/splunk/rba/issues.</li> <li>Create a new discussion at https://github.com/splunk/rba/discussions.</li> <li>Don't have a GitHub account? Reach out to us on Slack!</li> </ul>"},{"location":"contributing/contributing-guidelines/#how-to-contribute","title":"How to Contribute","text":"<p>This repository uses MkDocs with the Material for MkDocs theme.</p> <p>If you know the markdown language then using this style of documentation will be a breeze. For a full list of capabilities see MkDocs's website.</p>"},{"location":"contributing/contributing-guidelines/#fork-the-rba-github","title":"Fork the RBA GitHub","text":"<ul> <li>Fork the RBA GitHub page, make your changes, and submit a pull requests.</li> <li>Try to match the format of the existing documentation. <ul> <li>We can assist you with this process. </li> </ul> </li> </ul>"},{"location":"contributing/contributing-guidelines/#create-a-local-environment-for-testing","title":"Create a local environment for testing","text":"<p>Testing locally will be a great way to ensure your changes will work with what currently exists. </p> <p>The easiest way to get started is by using a python virtual environment. For simplicity, <code>pipenv</code> will be used for the following.</p> <ol> <li>Install python and pipenv on your local workstation -&gt; Pipenv docs.</li> <li> <p>Once installed, navigate to your forked repository and run the following to install the latest requirements.</p> <pre><code># your forked rba directory\n# ./rba\npipenv install -r docs/requirements.txt\n</code></pre> </li> <li> <p>Now you can enter <code>pipenv run mkdocs serve</code> which will create a webserver that can be reached by opening your browser and navigating to http://localhost:8000.</p> </li> </ol>"},{"location":"contributing/contributors/","title":"Thanks to our GitHub Contributors!","text":"<p> <code>7thdrxn</code> - Haylee Mills</p> <p> <code>ZachChristensen28</code></p> <p> <code>matt-snyder-stuff</code></p> <p> <code>ZachTheSplunker</code></p> <p> <code>nterl0k</code> - Steven Dick</p> <p> <code>hettervik</code></p> <p> <code>RedTigR</code></p> <p> <code>Dean Luxton</code></p> <p> <code>elusive-mesmer</code></p> <p> <code>gabs - Gabriel Vasseur</code></p> <p> <code>Daren Cook</code></p>"},{"location":"dashboards/","title":"Dashboards","text":""},{"location":"dashboards/#attck-matrix-risk-business-view","title":"ATT&amp;CK Matrix Risk (Business View)","text":"<p> attack_matrix_risk.xml</p> <p>Portrays risk in your environment through the lense of RBA and the MTRE ATT&amp;CK framework.</p>"},{"location":"dashboards/#attribution-analytics-tuning-view","title":"Attribution Analytics (Tuning View)","text":"<p> audit_attribution_analytics.xml</p> <p>Helpful for tuning new detections.</p>"},{"location":"dashboards/#rba-data-source-review","title":"RBA Data Source Review","text":"<p> rba_data_source_overview.xml</p> <p>This helps you to better what data sources you are using in RBA and see gaps in your coverage.</p>"},{"location":"dashboards/#risk-attributions-investigative-view","title":"Risk Attributions (Investigative View)","text":"<p> risk_attributions.xml</p> <p>Risk Attributions.</p>"},{"location":"dashboards/#risk-investigation","title":"Risk Investigation","text":"<p> risk_investigation.xml</p> <p>Risk Investigations.</p>"},{"location":"dashboards/#risk-notable-analysis","title":"Risk Notable Analysis","text":"<p> risk_notable_analysis_dashboard.xml</p> <p>Risk Notable Analysis.</p>"},{"location":"dashboards/attack_matrix_risk/","title":"ATT&amp;CK Matrix Risk (Business View)","text":"<p>View on GitHub </p> <p></p>"},{"location":"dashboards/audit_attribution_analytics/","title":"Attribution Analytics (Tuning View)","text":"<p>Helpful for tuning new detections.</p> <p>View on GitHub </p> <p></p>"},{"location":"dashboards/rba_data_source_overview/","title":"RBA Data Source Review","text":"<p>This helps you to better what data sources you are using in RBA and see gaps in your coverage.</p> <p>View on GitHub </p> <p></p>"},{"location":"dashboards/risk_attributions/","title":"Risk Attributions (Investigative View)","text":"Prerequisites <ul> <li>Install Network Diagram Viz: https://splunkbase.splunk.com/app/4438</li> <li>Add lookup file <code>attack_web_viz.csv</code>: https://github.com/apger/SA-RBA/blob/master/lookups/attack_web_viz.csv</li> </ul> <p>View on GitHub </p> <p></p>"},{"location":"dashboards/risk_investigation/","title":"Risk Investigation","text":"<p>View on GitHub </p> <p></p>"},{"location":"dashboards/risk_notable_analysis_dashboard/","title":"Risk Notable Analysis","text":"<p>View on GitHub </p> <p></p>"},{"location":"searches/","title":"Helpful Searches","text":"<p>These are some SPL techniques to get the most out of RBA by adding new features to your implementation or handling a common issue.</p>"},{"location":"searches/#integrate-ai-with-rir","title":"Integrate A&amp;I with RiR","text":"<p>Adding this SPL into your Risk Incident Rules normalizes your risk object to a unique key in the Asset &amp; Identity Framework; the primary advantage of this is throttling to prevent a Risk Incident Rule from firing on both a system and user that represent the same risk events.</p>"},{"location":"searches/#deduplicate-notables","title":"Deduplicate Notables","text":"<p>This feature will drastically reduce the number of duplicate Risk Notables by removing alerts where events are basically the same, already reviewed, or another Risk Incident Rule has already fired for.</p>"},{"location":"searches/#limit-score-stacking","title":"Limit score stacking","text":"<p>This SPL for your Risk Score Risk Incident Rules ensures that a single correlation search can only contribute risk a total of three times (or whatever you would like). This is handy for reducing rapidly stacking risk which is common early in the RBA maturation process.</p>"},{"location":"searches/#essential-rba-searches","title":"Essential RBA searches","text":"<p>This is all of the handy SPL contained in the Essential Guide to Risk Based Alerting; includes searches for finding noise, reducing noisy notables, and tuning risk rules.</p>"},{"location":"searches/#risk-info-field","title":"Risk info field","text":"<p>This is one of my favorite additions to RBA; adding this macro to your risk rules creates a field called risk_info (which you can add to your Risk Datamodel) containing all of the useful fields your analyst might use for analysis. It's in JSON formatting which allows easy manipulation in SPL and excellent material for dashboards and unique drilldowns per field.</p> <p>ADDITIONALLY, this frees risk_message to be used as a short and sweet summary rather than where you store all of the event detail. This lets Risk Notables tell a high level overview of events via risk_message, and is also handy to throttle or deduplicate by.</p>"},{"location":"searches/#chaining-behaviors","title":"Chaining behaviors","text":"<p>This is some simple SPL to organize risk events by risk_object and create risk rules which look for a specific sequence of events or chain of behaviors.</p>"},{"location":"searches/#naming-systemunknowncomputer-accounts","title":"Naming SYSTEM/Unknown/Computer Accounts","text":"<p>Computer accounts are used by Active Directory to authenticate machines to the domain, and RBA detections may find behavior in a log where the user account is simply listed as \"SYSTEM\" or even left blank because it is the computer account. This method renames the account to distinguish it as host$ from the noise of \"SYSTEM\" or \"unknown\". It can also be tied into the Asset &amp; Identify framework and contribute to detections on user risk objects.</p>"},{"location":"searches/asset_and_identity_rir_logic/","title":"Integrate Asset &amp; Identity Information into Risk Incident Rules","text":"<p>Note</p> <p>This feature has been added to ES 7.1, utilizing the <code>normalized_risk_object</code> field. This is also utilized for throttling in the default Risk Incident Rules which prevents notables from firing regularly on the same identity with different users or same asset with different hosts if A&amp;I is configured.</p> <p>This was a comment on this excellent Splunk Idea to <code>lower()</code> or <code>upper()</code> the risk_object in Risk Incident Rules, which goes one step further by integrating A&amp;I information:</p> <pre><code>| tstats `summariesonly` min(_time) as firstTime max(_time) as lastTime sum(All_Risk.calculated_risk_score) as risk_score, count(All_Risk.calculated_risk_score) as risk_event_count,values(All_Risk.annotations.mitre_attack.mitre_tactic_id) as annotations.mitre_attack.mitre_tactic_id, values(All_Risk.annotations.mitre_attack.mitre_technique_id) as annotations.mitre_attack.mitre_technique_id, values(All_Risk.tag) as tag, values(source) as source from datamodel=Risk.All_Risk by All_Risk.risk_object,All_Risk.risk_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| eval risk_object=upper(risk_object)\n| lookup update=true identity_lookup_expanded identity as risk_object OUTPUTNEW _key as asset_identity_id,identity as asset_identity_value\n| lookup update=true asset_lookup_by_str asset as risk_object OUTPUTNEW _key as asset_identity_id,asset as asset_identity_value\n| eval asset_identity_risk_object=CASE(isnull(asset_identity_id),risk_object,true(),asset_identity_id)\n| stats min(firstTime) as firstTime max(lastTime) as lastTime sum(risk_score) as risk_score, sum(risk_event_count) as risk_event_count,values(annotations.mitre_attack.mitre_tactic_id) as annotations.mitre_attack.mitre_tactic_id, dc(annotations.mitre_attack.mitre_tactic_id) as mitre_tactic_id_count, values(annotations.mitre_attack.mitre_technique_id) as annotations.mitre_attack.mitre_technique_id, dc(annotations.mitre_attack.mitre_technique_id) as mitre_technique_id_count, values(tag) as tag, values(source) as source, dc(source) as source_count values(asset_identity_value) as asset_identity_value values(risk_object) as risk_object dc(risk_object) as risk_object_count by asset_identity_risk_object,risk_object_type\n| eval \"annotations.mitre_attack\"='annotations.mitre_attack.mitre_technique_id', risk_threshold=100\n| eval user=case(risk_object_type=\"user\",risk_object,true(),user),src=case(risk_object_type=\"system\",risk_object,true(),src)\n| where risk_score &gt;= $risk_threshold$\n| `get_risk_severity(risk_score)`\n| `security_content_ctime(firstTime)`\n| `security_content_ctime(lastTime)`\n</code></pre> <p>Note</p> <p>As they mention in the comment -- the one \"catch\" is you'll need to change your throttle object from \"risk_object\" to \"asset_identity_risk_object\" -- but this is great for preventing duplicate notables on the same basic user / system combination.</p>"},{"location":"searches/asset_and_identity_rir_logic/#extra-credit","title":"Extra Credit","text":"<p>Adding the above logic will increase the accuracy of Risk based alerting, however pivoting via the built in drilldown will still be limited. The following changes will allow analysts to pivot directly to all Risk alerts detected by the assoicated RIR.</p> <p>Create a macro called <code>get_risk_asset_ident(2)</code> with the following.</p> Macro Definition<pre><code>eval risk_in=\"$risk_object_in$\",risk_type_in=\"$risk_object_type_in$\"\n| lookup update=true identity_lookup_expanded identity as risk_object OUTPUTNEW _key as assetid_ident_id,identity as assetid_ident_value | lookup update=true asset_lookup_by_str asset as risk_object OUTPUTNEW _key as assetid_asset_id,asset as assetid_asset_value | lookup update=true identity_lookup_expanded identity as risk_in OUTPUTNEW _key as assetid_in_ident,identity as assetid_in_ident_value | lookup update=true asset_lookup_by_str asset as risk_in OUTPUTNEW _key as assetid_in_asset,asset as assetid_in_asset_value | eval risk_object_out=CASE((risk_type_in=\"user\" AND assetid_ident_id = 'assetid_in_ident'),assetid_in_ident_value, (risk_type_in=\"system\" AND (assetid_asset_id = 'assetid_in_asset')),assetid_in_asset_value)\n| eval risk_in=upper(risk_in)\n| eval risk_object=upper(risk_object)\n| where isnotnull(risk_object_out) OR (risk_object = risk_in)\n</code></pre> Arguments<pre><code>risk_object_in,risk_object_type_in\n</code></pre> <p> </p> get_risk_asset_ident(2) completed macro. <p>Update macro permissions</p> <p>Assign global scope (All apps) and allow all users read permission.</p>"},{"location":"searches/asset_and_identity_rir_logic/#update-existing-rir-drilldowns","title":"Update existing RiR drilldowns","text":"<p>Modify existing RIR drilldowns to include the macro similar to below.</p> Example<pre><code>| from datamodel:\"Risk.All_Risk\"  | `get_risk_asset_ident($risk_object|s$,$risk_object_type|s$)`\n| `get_correlations`  | rename annotations.mitre_attack.mitre_tactic_id as mitre_tactic_id, annotations.mitre_attack.mitre_tactic as mitre_tactic, annotations.mitre_attack.mitre_technique_id as mitre_technique_id, annotations.mitre_attack.mitre_technique as mitre_technique\n</code></pre> <p>Authors</p> @7thdrxn - Haylee Mills"},{"location":"searches/deduplicate_notables/","title":"Deduplicate Notable Events","text":"<p>Throttle Alerts Which Have Already Been Reviewed or Fired</p> <p>Because Risk Notables look at a period of time, it is common for a risk_object to keep creating notables as additional (and even duplicate) events roll in, as well as when events fall off as the time period moves forward. Additionally, different Risk Incident Rules could be firing on the same risk_object with the same events but create new Risk Notables. It is difficult to get around this with throttling, so here are some methods to deduplicate notables.</p>"},{"location":"searches/deduplicate_notables/#navigation","title":"Navigation","text":"<p>Here are two methods for Deduplicating Notable Events:</p> - Skill Level Pros Cons Method I Intermediate Deduplicates on front and back end More setup time Method II Beginner Easy to get started with Only deduplicates on back end"},{"location":"searches/deduplicate_notables/#method-i","title":"Method I","text":"<p>We'll use a Saved Search to store each Risk Notable's risk events and our analyst's status decision as cross-reference for new notables. Altogether new events will still fire, but repeated events from the same source will not. This also takes care of duplicate notables on the back end as events roll off of our search window.</p> <p>KEEP IN MIND</p> <p>Edits to the Incident Review - Main search may be replaced on updates to Enterprise Security; requiring you to make this minor edit again to regain this functionality. Ensure you have a step in your relevant process to check this search after an update.</p>"},{"location":"searches/deduplicate_notables/#1-create-a-truth-table","title":"1. Create a Truth Table","text":"<p>This method is described in Stuart McIntosh's 2019 .conf Talk (about 9m10s in), and we're going to create a similar lookup table. You can either download and import that file yourself, or create something like this in the Lookup Editor app:</p> <p> </p> Truth Table"},{"location":"searches/deduplicate_notables/#2-create-a-saved-search","title":"2. Create a Saved Search","text":"<p>Then we'll create a Saved Search which runs relatively frequently to store notable data and statuses.</p> <ol> <li>Navigate to Settings -&gt; Searches, reports, and alerts.</li> <li>Select \"New Report\" in the top right.</li> </ol> <p>Here is a sample to replicate</p> <p> </p> Sample Report With this SPL<pre><code>index=notable eventtype=risk_notables\n| eval indexer_guid=replace(_bkt,\".*~(.+)\",\"\\1\"),event_hash=md5(_time._raw),event_id=indexer_guid.\"@@\".index.\"@@\".event_hash\n| fields _time event_hash event_id risk_object risk_score source orig_source\n| eval temp_time=time()+86400\n| lookup update=true event_time_field=temp_time incident_review_lookup rule_id AS event_id OUTPUT status as new_status\n| lookup update=true correlationsearches_lookup _key as source OUTPUTNEW default_status\n| eval status=case(isnotnull(new_status),new_status,isnotnull(status),status,1==1,default_status)\n| fields - temp_time,new_status,default_status\n| eval temp_status=if(isnull(status),-1,status)\n| lookup update=true reviewstatuses_lookup _key as temp_status OUTPUT status,label as status_label\n| fields - temp_status\n| eval sources = if(isnull(sources) , orig_source , sources )\n| table _time event_hash risk_object source status_label sources risk_score\n| reverse\n| streamstats current=f window=0 latest(event_hash) as previous_event_hash values(*) as previous_* by risk_object\n| eval previousNotable=if(isnotnull(previous_event_hash) , \"T\" , \"F\" )\n| fillnull value=\"unknown\" previous_event_hash previous_status_label previous_sources previous_risk_score\n| eval matchScore = if( risk_score != previous_risk_score , \"F\" , \"T\" )\n| eval previousStatus = case( match(previous_status_label, \"(Closed)\") , \"nonmalicious\" , match(previous_status_label, \"(New|Resolved)\") , \"malicious\" , true() , \"malicious\" )\n# (1)!\n| mvexpand sources\n| eval matchRR = if(sources != previous_sources , \"F\", \"T\")\n| stats  dc(sources) as dcSources dc(matchRR) as sourceCheckFlag values(*) as * by _time risk_object event_hash\n| eval matchRR = if(sourceCheckFlag &gt; 1 , \"F\" , matchRR )\n| lookup RIR-Truth-Table.csv previousNotable previousStatus matchRR matchScore OUTPUT alert\n| table _time risk_object source risk_score event_hash dcSources alert previousNotable previousStatus matchRR matchScore\n| outputlookup RIR-Deduplicate.csv\n</code></pre> <ol> <li><code>previousStatus</code> uses the default ES status label \"Closed\".</li> </ol> <p>In the SPL for <code>previousStatus</code> above, I used the default ES status label \"Closed\" as our only nonmalicious status. You'll have to make sure to use status labels which are relevant for your Incident Review settings. \"Malicious\" is used as the fallback status just in case, but you may want to differentiate \"New\" or unmatched statuses as something else for audit purposes; just make sure to create relevant matches in your truth table.</p> <p>I recommend copying the alert column from malicious events</p>"},{"location":"searches/deduplicate_notables/#schedule-the-saved-search","title":"Schedule the Saved Search","text":"Create schedule<pre><code>    Now find the search in this menu, click *Edit -&gt; Edit Schedule* and try these settings:\n</code></pre> <ul> <li>Schedule: Run on Cron Schedule</li> <li>Cron Expression: <code>*/3 * * * *</code></li> <li>Time Range: Last 7 days</li> <li>Schedule Priority: Highest</li> <li>Schedule Window: No window</li> </ul> <p>I made this search pretty lean, so running it every three minutes should work pretty well; I also decided to only look back seven days as this lookup could balloon in size and cause bundle replication issues. You probably want to stagger your Risk Incident Rule cron schedules by one minute more than this one so they don't fire on the same risk_object with the same risk events.</p>"},{"location":"searches/deduplicate_notables/#3-deduplicate-notables","title":"3. Deduplicate notables","text":"<p>Our last step is to ensure that the Incident Review panel doesn't show us notables when we've found a match to our truth table which doesn't make sense to alert on. In the Searches, reports, alerts page, find the search Incident Review - Main and click Edit -&gt; Edit Search.</p> <p>By default it looks like this:</p> <p> Default incident review search <p>And we're just inserting this line after the base search</p> Append to the base search<pre><code>...\n| lookup RIR-Deduplicate.csv _time risk_object source OUTPUTNEW alert\n| search NOT alert=\"no\"\n</code></pre> <p> </p> Updated incident review search"},{"location":"searches/deduplicate_notables/#congratulations","title":"Congratulations!","text":"<p>You should now have a significant reduction in duplicate notables</p> <p>If something isn't working, make sure that the Saved Search is correctly outputting a lookup (which should have Global permissions), and ensure if you <code>| inputlookup RIR-Deduplicate.csv</code> you see all of the fields being returned as expected. If Incident Review is not working, something is wrong with the lookup or your edit to that search.</p>"},{"location":"searches/deduplicate_notables/#extra-credit","title":"Extra Credit","text":"<p>If you utilize the Risk info field so you have a short and sweet risk_message, you can add another level of granularity to your truth table.</p> <p>if you utilize risk_message for ALL of the event detail, it may be too granular and isn't as helpful for throttling.</p> <p>This is especially useful if you are creating risk events from a data source with its own signatures like EDR, IDS, or DLP. Because the initial truth table only looks at score and correlation rule, if you have one correlation rule importing numerous signatures, you may want to alert when a new signature within that source fires.</p>"},{"location":"searches/deduplicate_notables/#create-a-calculated-field","title":"Create a calculated field","text":"<p>First, we'll create a new Calculated Field from risk_message in our Risk Datamodel called risk_hash with eval's <code>md5()</code> function, which bypasses the need to deal with special characters or other strangeness that might be in that field. If you haven't done this before - no worries - you just have to go to Settings -&gt; Data Models -&gt; Risk Data Model -&gt; Edit -&gt; Edit Acceleration and turn this off. Afterwards, you can Create New -&gt; Eval Expression like this:</p> <p> </p> Creating risk_hash from md5(risk_message) in data model Don't forget to re-enable the acceleration <p>You may have to rebuild the data model from the Settings -&gt; Data Model menu for this field to appear in your events.</p>"},{"location":"searches/deduplicate_notables/#update-spl","title":"Update SPL","text":"<p>Then we have to add this field into our Risk Incident Rules by adding this line to their initial SPL and ensure this field is retained downstream:</p> Field to add to RiR<pre><code>values(All_Risk.risk_hash) as risk_hashes\n</code></pre> <p>Now our Risk Notables will have a multi-value list of <code>risk_message</code> hashes. We must update our truth table to include a field called \"matchHashes\" - I've created a sample truth table here, but you must decide what is the proper risk appetite for your organization.</p> <p>Next we'll edit the Saved Search we created above to include the new fields and logic:</p> Updated logic (changes highlighted)<pre><code>...\n| eval sources = if(isnull(sources) , orig_source , sources )\n| table _time event_hash risk_object source status_label sources risk_score risk_hashes\n| reverse\n| streamstats current=f window=0 latest(event_hash) as previous_event_hash values(*) as previous_* by risk_object\n| eval previousNotable=if(isnotnull(previous_event_hash) , \"T\" , \"F\" )\n| fillnull value=\"unknown\" previous_event_hash previous_status_label previous_sources previous_risk_score previous_risk_hashes\n| eval matchScore = if( risk_score != previous_risk_score , \"F\" , \"T\" )\n| eval previousStatus = case( match(previous_status_label, \"(Closed)\") , \"nonmalicious\" , match(previous_status_label, \"(New|Resolved)\") , \"malicious\" , true() , \"malicious\" )\n| mvexpand risk_hashes\n| eval matchHashes= if(risk_hashes != previous_risk_hashes , \"F\" , \"T\" )\n| stats dc(matchHashes) as hashCheckFlag values(*) as * by _time risk_object event_hash\n| eval matchHashes = if(hashCheckFlag &gt; 1 , \"F\" , matchHashes )\n| mvexpand sources\n| eval matchRR = if(sources != previous_sources , \"F\", \"T\")\n| stats  dc(sources) as dcSources dc(matchRR) as sourceCheckFlag values(*) as * by _time risk_object event_hash\n| eval matchRR = if(sourceCheckFlag &gt; 1 , \"F\" , matchRR )\n| lookup RIR-Truth-Table.csv previousNotable previousStatus matchRR matchScore matchHashes OUTPUT alert\n| table _time risk_object source risk_score event_hash dcSources alert previousNotable previousStatus matchRR matchScore matchHashes\n| outputlookup RIR-Deduplicate.csv\n</code></pre> <p>Voila! We now ensure that our signature-based risk rule data sources will properly alert if there are interesting new events for that risk object.</p>"},{"location":"searches/deduplicate_notables/#method-ii","title":"Method II","text":"<p>This method is elegantly simple to ensure notables don't re-fire as earlier events drop off the rolling search window of your Risk Incident Rules. It does this by only firing if the latest risk event is from the past 70 minutes.</p> Append to existing RIR<pre><code>...\n| stats latest(_indextime) AS latest_risk\n| where latest_risk &gt;= relative_time(now(),\"-70m@m\")\n</code></pre> <p>Credit to Josh Hrabar and Josh Campbell, this is brilliant. Thanks y'all!</p> <p>Authors</p> @7thdrxn - Haylee Mills"},{"location":"searches/dynamic_drilldowns/","title":"Dynamic Drilldowns by Source","text":"<p>If you're utilizing a custom risk notable investigation dashboard, it can be incredibly helpful for each risk event source to have its own drilldown. Thanks to Donald Murchison from the RBA Slack for contributing this method, which is explained in more detail in this blog post.</p>"},{"location":"searches/dynamic_drilldowns/#create-a-drilldown-lookup","title":"Create a Drilldown Lookup","text":"<p>First you'll need a lookup file with your risk rule name, the drilldown itself, and a description, like so:</p>    ![specific drilldowns will help analysts find exactly what they want to know](../assets/dashboard_drilldown_lookup.png)   specific drilldowns will help analysts find exactly what they want to know <p>You can utilize this example from Donald's article and change it to suit your purposes.</p>"},{"location":"searches/dynamic_drilldowns/#create-your-drilldown-panel","title":"Create your Drilldown Panel","text":"<p>In Donald's example, this panel shows the list of sources for the risk object indicated by $risk_object_token$ (which you will need to ensure matches whatever token your dashboard uses), a description, and the drilldown logic itself. Here is the SPL and helpful comments:</p> Drilldown Panel SPL<pre><code>| tstats summariesonly=false count from datamodel=Risk.All_Risk where All_Risk.risk_object=\"$risk_object_token$\" by source\n``` Get a list of all risk rules that have generated a risk event for this risk object - assumes the dashbaord has an input which stores risk_object in \"risk_object_token\"\nreplace risk_object_token with your own token name - helpful to use risk_object_type in search if this is in a token as well ```\n| fields source\n``` Use map to run a search for each risk rule to generate the drilldowns - map was used to be able to pass the risk rule name as an argument to the subsearch.\nThis is required because we must run an individual \"\n| eval drilldown=\u2026\" for each risk rule in case fields are used in the drilldown that do not exist in other risk events.\nString concatentation with a null field would make our entire string null.\nIf you wanted to remove map for better performance you could do this by only using fields that are present in every risk rule or building drilldowns with coalesce - coalesce(risk_object,\\\"\\\") - to ensure no null fields```\n| map search=\"index=risk risk_object=\\\"$risk_object_token$\\\" | eval drilldown=[| inputlookup rba_risk_rule_drilldowns.csv | eval search_name=split(search_name,\\\"|\\\") | search search_name=\\\"$$source$$\\\" | eval drilldown=\\\"\\\\\\\"\\\".search_name.\\\"||@||\\\\\\\".\\\".drilldown.\\\".\\\\\\\"||@||\\\".description.\\\"\\\\\\\"\\\"\n``` In the map search, we first search for all risk events related to the risk rule. Every risk event will get a drilldown field that we will dedup later. We do not use the datamodel in case fields outside of the datamodel are used in the drilldown.\nThe |inputlookup subsearch concatenates search_name, drilldown, and description for each row```\n| stats values(drilldown) as drilldown\n| eval drilldown=mvjoin(drilldown,\\\".\\\\\\\"||&amp;||\\\\\\\".\\\")\n``` We then condense all drilldowns to a single field and concatenate together - this allows us to evaluate all drilldowns within a single eval statement```\n|return $drilldown] | fields drilldown\"\n```Now we break out the drilldowns into their respective components```\n| eval drilldown=split(drilldown,\"||&amp;||\")\n| mvexpand drilldown\n| eval search_name=mvindex(split(drilldown,\"||@||\"),0)\n| eval drilldown_description=mvindex(split(drilldown,\"||@||\"),2)\n| eval drilldown=mvindex(split(drilldown,\"||@||\"),1)\n| stats values(*) as * by drilldown\n``` Use stats to dedup the drilldowns - depending on the fields used in the drilldown there could be multiple variations of the same drilldown```\n| table search_name drilldown_description drilldown\n</code></pre>"},{"location":"searches/dynamic_drilldowns/#add-drilldown-functionality","title":"Add Drilldown Functionality","text":"<p>You can follow along with Donald's article to add the drilldown in the GUI editor, but the SimpleXML for this panel would be:</p> Drilldown SimpleXML<pre><code>&lt;drilldown&gt;\n&lt;link target=\"_blank\"&gt;search?q=$row.drilldown$&amp;amp;earliest=$field1.earliest$&amp;amp;latest=$field1.latest$&lt;/link&gt;\n&lt;/drilldown&gt;\n</code></pre> <p>For a click anywhere on that row to drive the search. Make sure your timepicker tokens match here as well!</p> <p>You could also utilize the time around an event by retaining <code>_time</code> in the initial search and declaring this later in SPL: Extra Time Control<pre><code>| eval lowtime = _time - 300\n| eval hightime = _time + 300\n</code></pre></p> <p>So you could use <code>$row.lowtime$</code> and <code>$row.hightime$</code> for your drilldown and search a five minute window around an event instead of utilizing the standard time picker for your dashboard.</p>"},{"location":"searches/dynamic_drilldowns/#customize-your-heart-out","title":"Customize Your Heart Out","text":"<p>This is a great way to incorporate something akin to Workflow Actions for your custom dashboards. You could go a bit further and potentially:</p> <ul> <li>Include <code>risk_message</code> in your drilldown panel for more context</li> <li>Hide the drilldown field in the panel to save real estate with <code>&lt;fields&gt;</code> in SimpleXML</li> <li>Utilize <code>case(isnull(drilldown)</code> logic to utilize generalized drilldowns in case nothing is explicitly defined</li> </ul> <p>Just as examples. Please share your variations on the RBA Slack!</p>"},{"location":"searches/limit_score_stacking/","title":"Limit Risk Rule Score Stacking","text":"<p>These will help reduce the maximum amount of risk which can be added from noisy Risk Rules.</p>"},{"location":"searches/limit_score_stacking/#navigation","title":"Navigation","text":"<p>There are two methods for limiting score stacking</p> - Skill Level Pros Cons Method I Beginner Easy to get started with Less context around what was capped and why Method II Intermediate More precise deduplication and additional information Additional understanding of SPL"},{"location":"searches/limit_score_stacking/#method-i","title":"Method I","text":"<p>This caps the risk score contribution of a single source by 3x the highest score from that source.</p> <pre><code>| tstats summariesonly=true sum(All_Risk.calculated_risk_score) as summed_risk_score max(All_Risk.calculated_risk_score) as single_risk_score dc(source) as source_count count\n FROM datamodel=Risk.All_Risk\n WHERE All_Risk.risk_object_type=\"*\" (All_Risk.risk_object=\"*\" OR risk_object=\"*\")\nBY All_Risk.risk_object All_Risk.risk_object_type source\n| eval capped_risk_score=if(summed_risk_score &lt; single_risk_score*3, summed_risk_score, single_risk_score*3)\n| stats sum(capped_risk_score) as capped_risk_score sum(summed_risk_score) as summed_risk_score dc(source) as source sum(count) as count\n BY All_Risk.risk_object All_Risk.risk_object_type\n| sort 1000 - risk_score\n...\n</code></pre> <p>Note</p> <p>You may want to limit this to particular sources, but this is extra handy for noisy sources like EDR, DLP, or IDS.</p> <p>Thanks David Dorsey!</p>"},{"location":"searches/limit_score_stacking/#method-ii","title":"Method II","text":"<p>This option adds some complexity, however, provides more information and better deduplication. The full write-up of how to accomplish this method can be found on gabs website.</p> <p>Visit Website </p> <p> *reference: https://www.gabrielvasseur.com/post/rba-a-better-way-to-dedup-risk-events</p> Final SPL from blog post<pre><code>| inputlookup TEMP_GABS_riskybusiness.csv\n``` First we take the breakdown of what actually happened, before doing any kind of deduping ```\n| eventstats sum(count) as count_msg\n    by risk_object risk_object_type risk_score source risk_message ```Get breakdown per risk_message``` \n| eventstats values(eval(count_msg.\"*\".risk_score)) as breakdown_msg\n    by risk_object risk_object_type            source risk_message ```Get breakdown per risk_message```\n| eventstats sum(count) as count_src\n    by risk_object risk_object_type risk_score source ```Get breakdown per source```\n| eventstats values(eval(count_src.\"*\".risk_score)) as breakdown_src\n    by risk_object risk_object_type            source ```Get breakdown per source```\n| stats sum(count) as risk_event_count, values(breakdown_src) as breakdown_src,\n    values(breakdown_msg) as breakdown_msg, sum(eval(risk_score*count)) as total_score,\n    max(risk_score) as max_score, latest(_time) as _time, values(mitre_*) as mitre_*\n    by risk_object risk_object_type source risk_message ```Reduce to unique risk_message\n    (it's not impossible to have several risks with the same risk_message but different scores)```\n| eval risk_message= mvjoin(breakdown_msg,\"+\").\"=\".max_score\n    . if( total_score!=max_score, \" (!\" . total_score . \")\", \"\") . \" \" .risk_message\n``` START limit to a maximum of 10 contributions per source ```\n| sort 0 risk_object risk_object_type source - max_score ``` Only the lowest scores will be dedup'd ```\n| eventstats dc(risk_message) as dc_msg_per_source by risk_object risk_object_type source \n| streamstats count as rank_per_source by risk_object risk_object_type source \n| eval risk_message=case( \n    rank_per_source &lt;= 10, risk_message,\n    rank_per_source = 11, \"...+\" . ( dc_msg_per_source - 20 ) . \" others from '\" . source . \"'...\" ,\n    1==1, null() ) \n| eval max_score=if( rank_per_source &lt;= 10, max_score, 0 )\n``` END limit to a maximum of 10 contributions per source ```\n| stats sum(risk_event_count) as risk_event_count, values(breakdown_src) as breakdown_src,\n    list(risk_message) as risk_message, sum(max_score) as risk_score,\n    sum(total_score) as risk_score_nodedup, latest(_time) as _time, values(mitre_*) as mitre_*\n    by risk_object risk_object_type source ```Reduce to unique source```\n| eval breakdown_src = mvjoin(breakdown_src,\"+\") .\"=\".risk_score\n    . if( risk_score!=risk_score_nodedup, \" (!\" . risk_score_nodedup . \")\", \"\" ) . \" \".source\n| stats sum(risk_event_count) as risk_event_count, list(source) as source, dc(source) as source_count,\n    list(breakdown_src) as srcs, list(risk_message) as risk_message, sum(risk_score) as risk_score,\n    sum(risk_score_nodedup) as risk_score_nodedup, latest(_time) as _time, values(mitre_*) as mitre_*,\n    dc( mitre_tactic_id) as mitre_tactic_id_count, dc(mitre_technique_id) as mitre_technique_id_count\n    by risk_object risk_object_type ```Reduce to unique object```\n</code></pre> <p>Authors</p> @7thdrxn - Haylee Mills @gabs - Gabriel Vasseur"},{"location":"searches/naming_system_unknown_computer_accounts/","title":"Naming SYSTEM / Unknown / Computer Accounts - The SEAL Method","text":"<p>Computer accounts are used by Active Directory to authenticate machines to the domain, and RBA detections may find behavior in a log where the user account is simply listed as \"SYSTEM\" or even left blank because it is the computer account. This method renames the account to distinguish it as host$ from the noise of \"SYSTEM\" or \"unknown\". It can also be tied into the Asset &amp; Identify framework and contribute to detections on user risk objects.</p>"},{"location":"searches/naming_system_unknown_computer_accounts/#steps","title":"Steps","text":"<p>Navigate to Settings &gt; Fields &gt; Calculated Fields &gt; Add New</p> Setting Value Source <code>XmlWinEventLog:Security</code> Source <code>XmlWinEventLog:Microsoft-Windows-Sysmon/Operational</code> Name <code>user</code> Eval Expression <code>if(user=\"SYSTEM\" OR user=\"-\",'host'+\"$\",'user')</code> Conflicting knowledge objects - Sysmon TA <p>We have to be careful with existing order of knowledge objects and calculated fields. The Sysmon TA already has a <code>user = \"\"</code> calculated field which we can update as follows:</p> Existing:<pre><code>user = upper(case(\nNOT isnull(User) AND NOT User IN (\"-\"), replace(User, \"(.*)\\\\\\(.+)$\",\"\\2\"),\n    NOT isnull(SourceUser) AND NOT isnull(TargetUser) AND SourceUser==TargetUser, replace(SourceUser, \"(.*)\\\\\\(.+)$\",\"\\2\")\n))\n</code></pre> Update to:<pre><code>user = upper(case(\nmatch(User,\".+\\\\\\SYSTEM\"), host.\"$\",\n    NOT isnull(User) AND NOT User IN (\"-\"), replace(User, \"(.*)\\\\\\(.+)$\",\"\\2\"),\n    NOT isnull(SourceUser) AND NOT isnull(TargetUser) AND SourceUser==TargetUser, replace(SourceUser, \"(.*)\\\\\\(.+)$\",\"\\2\")\n))\n</code></pre>"},{"location":"searches/naming_system_unknown_computer_accounts/#extra-credit","title":"Extra Credit","text":"<p>Not going to map this entire process due to how different it can be in each environment, but you can now add the computer account to your Identity lookup to aggregate with other user accounts. For example, you might take the fields <code>nt_host</code> and <code>owner</code> from your Asset lookup (asset_lookup_by_str), then map <code>owner</code> to <code>email</code> in the Identity lookup (identity_lookup_expanded). If you make a saved search that outputs a CSV, you can now use that to add fields into your Identity lookup.</p> <p>Authors</p> @Dean Luxton @StevenD"},{"location":"searches/risk_guide_searches/","title":"Essential RBA searches","text":"<p>Handy SPL contained in the Essential Guide to Risk Based Alerting.</p>"},{"location":"searches/risk_guide_searches/#determine-correlation-searches-with-high-falsebenign-positive-rates","title":"Determine Correlation Searches with High False/Benign Positive Rates","text":"<pre><code>`notable`\n| stats count(eval(status_label=\"Incident\")) as incident count(eval(status_label=\"Resolved\")) as closed\n BY source\n| eval benign_rate = 1 - incident / (incident + closed)\n| sort - benign_rate\n</code></pre> Note <p>Be sure to replace the <code>status_label</code> with whatever is used in your environment.</p>"},{"location":"searches/risk_guide_searches/#risk-rules-generating-the-most-risk","title":"Risk Rules Generating the Most Risk","text":"<pre><code>| tstats summariesonly=false sum(All_Risk.calculated_risk_score)\nas risk_score,dc(All_Risk.risk_object)\nas risk_objects,count\n FROM datamodel=Risk.All_Risk\n WHERE * All_Risk.risk_object_type=\"*\" (All_Risk.risk_object=\"*\" OR risk_object=\"*\")\nBY source\n| sort 1000 - count risk_score\n</code></pre>"},{"location":"searches/risk_guide_searches/#dig-into-noisy-threat-objects","title":"Dig into Noisy Threat Objects","text":"<pre><code>| tstats summariesonly=true count dc(All_Risk.risk_object) as dc_objects dc(All_Risk.src) as dc_src dc(All_Risk.dest) as dc_dest dc(All_Risk.user) as dc_users dc(All_Risk.user_bunit) as dc_bunit sum(All_Risk.calculated_risk_score) as risk_score values(source) as source\nFROM datamodel=Risk.All_Risk\n BY All_Risk.threat_object,All_Risk.threat_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| sort 1000 - risk_score\n</code></pre>"},{"location":"searches/risk_guide_searches/#find-noisiest-risk-rules-in-risk-notables","title":"Find Noisiest Risk Rules in Risk Notables","text":"<pre><code>index=notable eventtype=risk_notables\n| stats count\n BY orig_source\n| eventstats sum(count) as total\n| eval percentage = round((count / total) * 100,2)\n| sort - percentage\n</code></pre>"},{"location":"searches/risk_guide_searches/#structural-changes","title":"Structural Changes","text":""},{"location":"searches/risk_guide_searches/#notable-macro-to-edit-for-qa-risk-notables","title":"Notable Macro to Edit for QA Risk Notables","text":"<p>Add <code>| eval QA=1</code> to the end of your Risk Incident Rules, editing the macro <code>get_notable_index</code> from the default to begin \"QA\" mode.</p> default<pre><code>index=notable\n</code></pre> QA mode<pre><code>index=notable NOT QA=1\n</code></pre> <p>This will keep Risk Notables out of your Incident Review queue while you develop RBA.</p>"},{"location":"searches/risk_guide_searches/#create-a-sandbox-for-risk-rules-away-from-risk-notables","title":"Create a Sandbox for Risk Rules away from Risk Notables","text":"<p>Create an eventtype called something like <code>QA</code> and have it apply a tag called <code>QA</code>, then add the following to your Risk Incident Rules.</p> <pre><code>...\nWHERE NOT All_Risk.tag=QA\n...\n</code></pre> <p>This keeps your curated risk ecology preserved so you can compare how many Risk Notables you would see if your QA content was added.</p>"},{"location":"searches/risk_guide_searches/#include-previous-notables-in-new-notables","title":"Include Previous Notables in New Notables","text":"<p>If you create a lookup from a saved search called <code>Past7DayNotables.csv</code> where you store the previous time, status, and sources, you could include this in your Risk Incident Rules:</p> <pre><code>| lookup Past7DayNotables.csv risk_object OUTPUT prev_time prev_status prev_sources\n| eval prev_alerts = prev_time.\" - \".prev_status.\" - \".prev_sources\n</code></pre> Note <p>Make sure to add <code>prev_alerts</code> to the Incident Review Settings page so this shows up in the Incident Review panel.</p>"},{"location":"searches/risk_guide_searches/#tuning","title":"Tuning","text":""},{"location":"searches/risk_guide_searches/#remove-results-with-a-lookup","title":"Remove Results with a Lookup","text":"<p>Once you have a lookup built out, insert it into a search like this:</p> <pre><code>index=proxy http_method=\"POST\" NOT\n  [| inputlookup RR_Proxy_Allowlist.csv\n  | fields Web.src Web.dest\n  | rename Web.* AS *]\n</code></pre> <p>You could also do this with a datamodel:</p> <pre><code>| tstats summariesonly=t values(Web.dest) as dest from datamodel Web.Web where Web.http_method=\"POST\" NOT\n  [| inputlookup RR_Proxy_Allowlist.csv | fields Web.src Web.dest]\nby _time, Web.src\n</code></pre> <p>Using the Web datamodel field constraints as an example so we can properly exclude results from index or datamodel based risk rules.</p>"},{"location":"searches/risk_guide_searches/#adjust-risk-scores","title":"Adjust Risk Scores","text":"<ul> <li>Using eval</li> <li>Using lookup</li> </ul>"},{"location":"searches/risk_guide_searches/#using-eval","title":"Using <code>eval</code>","text":"<pre><code>index=proxy signature=*\n| table src user user_bunit dest signature http_code\n| eval risk_score = case(\nsignature=\"JS:Adware.Lnkr.A\",\"10\",\n  signature=\"Win32.Adware.YTDownloader\",\"0\",\n  NOT http_code=\"200\",\"25\",\n  signature=\"Trojan.Win32.Emotet\" AND NOT user_bunit=\"THREAT INTELLIGENCE\",\"100\"\n)\n</code></pre> <p>In this example, we are:</p> <ul> <li>Assigning the score of 10 for a signature that isn't generally bad but we still want to add a small amount of risk.</li> <li>Zeroing out the score for a signature of something a lot of our users have installed and we can't really control, but still want to observe is happening.</li> <li>Assigning the score of 25 for an unsuccessful HTTP connection.</li> <li>Assigning the score of 100 and potentially alerting directly in case we see malware from someone who is not on the Threat Intelligence team.</li> <li>Assigning a null() value in every other case to utilize the default risk score from the Risk Analysis action.</li> </ul>"},{"location":"searches/risk_guide_searches/#using-lookup","title":"Using <code>lookup</code>","text":"<pre><code>index=proxy signature=*\n| table src user user_bunit dest signature http_code\n| lookup RR_Proxy_Adjust.csv src user user_bunit dest signature http_code OUTPUTNEW risk_score\n</code></pre> <p>We can do the same with a lookup and as many relevant fields as we need for the most constrained exclusions.</p>"},{"location":"searches/risk_guide_searches/#dedup-similar-events-from-counting-multiple-times-in-risk-notables-score","title":"Dedup Similar Events from Counting Multiple Times in Risk Notables (Score)","text":"<pre><code>...\n BY All_Risk.risk_object,All_Risk.risk_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| streamstats sum(risk_score) as original_score values(source) as sources values(risk_message) as risk_messages by risk_object\n| eval adjust_score = case(\nsource IN (\"My Noisy Rule That Fires a Lot but I Still Want to Know About, Once\", \"My Other Really Useful Context Low Risk Rule\"),\"1\",\n match(risk_message,\"IDS - Rule Category 1.*|IDS - Rule Category 2.*\") OR match(risk_message,\"DLP - Rule Category 1.*|DLP - Rule Category 2.*\"),\"1\",\n 1=1,null())\n| eval combine = coalesce(adjust_score,risk_message)\n| dedup combine risk_score\n| streamstats sum(risk_score) as risk_score values(sources) as source values(risk_messages) as risk_message by risk_object\n...\n</code></pre> <p>For making sure similar detections on basically the same event only count once in our total risk score.</p>"},{"location":"searches/risk_guide_searches/#weight-events-from-noisy-sources-in-risk-notables-metadata","title":"Weight Events from Noisy Sources in Risk Notables (Metadata)","text":"<pre><code>...\nBY All_Risk.risk_object,All_Risk.risk_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| mvexpand source\n| lookup RIRadjust-rule_weight.csv source OUTPUTNEW mitre_weight source_weight\n| eval mitre_weight = if(isnotnull(mitre_weight),mitre_weight,\"0\")\n| eval source_weight = if(isnotnull(source_weight),source_weight,\"0\")\n| streamstats sum(mitre_weight) as mitre_weight_total sum(source_weight) as source_weight_total values(*) as * by risk_object risk_object_type\n| eval mitre_tactic_id_count = mitre_tactic_id_count - mitre_weight_total\n| eval source_count = source_count - source_weight_total\n| eval \"annotations.mitre_attack\" = 'annotations.mitre_attack.mitre_technique_id'\n| where mitre_tactic_id_count &gt;= 3 and source_count &gt;= 4\n</code></pre> <p>For tuning Risk Incident Rules that don't rely on an accretive score to alert, but still need a lever to tweak noisy sources. In our example lookup, we would include a value between 0 and 1 for each noisy source; IE 0.75 to only count a rule as \u00bc of a standard weight, 0.5 to only count as \u00bd, etc.</p> <p>Authors</p> @7thdrxn - Haylee Mills"},{"location":"searches/risk_incident_rule_ideas/","title":"Risk Incident Rule Ideas","text":"<p>Here are some alternative ways to alert from the risk index that you may find useful. Later searches will be relying on the base search found in the \"Capped Risk Score by Source\" approach.</p> - Description Capped Risk Score by Source From the [limit score stacking] approach(https://github.com/splunk/rba/blob/main/docs/searches/limit_score_stacking.md) Events from Multiple Sourcetypes For events from multiple sourcetypes Events from Multiple Sourcetypes with Meta-Scoring Similar, but with more control over what alerts and how MITRE Counts with Meta-Scoring Meta-scoring approach to MITRE alert"},{"location":"searches/risk_incident_rule_ideas/#capped-risk-score-by-source","title":"Capped Risk Score by Source","text":"<p>Utilizes the [limit score stacking] approach(https://github.com/splunk/rba/blob/main/docs/searches/limit_score_stacking.md) to limit score contribution from a single source to double of its highest scoring risk event.</p> <pre><code>| tstats `summariesonly`\ncount as count\nsum(All_Risk.calculated_risk_score) as risk_score,\ncount(All_Risk.calculated_risk_score) as risk_event_count,\nsum(All_Risk.calculated_risk_score) as summed_risk_score,\nmax(All_Risk.calculated_risk_score) as single_risk_score,\nvalues(All_Risk.risk_message) as risk_message,\nvalues(All_Risk.annotations.mitre_attack.mitre_tactic_id) as annotations.mitre_attack.mitre_tactic_id,\ndc(All_Risk.annotations.mitre_attack.mitre_tactic_id) as mitre_tactic_id_count,\nvalues(All_Risk.annotations.mitre_attack.mitre_technique_id) as annotations.mitre_attack.mitre_technique_id,\ndc(All_Risk.annotations.mitre_attack.mitre_technique_id) as mitre_technique_id_count,\nvalues(All_Risk.tag) as tag,\nvalues(All_Risk.threat_object) as threat_object,\nvalues(All_Risk.threat_object_type) as threat_object_type,\ndc(source) as source_count,\n,max(_time) as _time\nfrom datamodel=Risk.All_Risk by All_Risk.risk_object,All_Risk.risk_object_type, source | `drop_dm_object_name(\"All_Risk\")` | eval \"annotations.mitre_attack\"='annotations.mitre_attack.mitre_technique_id' | `get_risk_severity(risk_score)`\n| eval capped_risk_score=if(summed_risk_score &lt; single_risk_score*2, summed_risk_score, single_risk_score*2)\n| stats values(*) as * sum(capped_risk_score) as capped_risk_score sum(summed_risk_score) as summed_risk_score dc(annotations.mitre_attack.mitre_tactic_id) as mitre_tactic_id_count dc(annotations.mitre_attack.mitre_technique_id) as mitre_technique_id_count sum(risk_event_count) as risk_event_count dc(source) as source_count\n BY risk_object risk_object_type\n| fields - single_risk_score count\n| eval risk_score = summed_risk_score\n| where capped_risk_score &gt; 100\n</code></pre>"},{"location":"searches/risk_incident_rule_ideas/#events-from-multiple-sourcetypes","title":"Events from Multiple Sourcetypes","text":"<p>This is a very effective approach that looks for when a single risk object has events from multiple security data sources. With a well-defined naming scheme for your searches, you may not need to utilize a saved search to retain this information in your risk rules. If you do, you could run something like this somewhat infrequently as a saved search:</p> <pre><code>| rest splunk_server=local count=0 /services/saved/searches\n| search action.correlationsearch.enabled=1\n| rename dispatch.earliest_time as early_time qualifiedSearch as search_spl\n| table title search_spl\n| eval data_sourcetype = case(\nmatch(search_spl,\".*\\`(sysmon|wmi|powershell|wineventlog_(security|system))\\`.*\") OR match(search_spl,\".*datamodel(:|=|\\s)(|\\\")Endpoint.*\") OR match(title,\"Endpoint.*\") OR match(search_spl,\".*sourcetype\\=(|\\\")(xmlwineventlog:microsoft-windows-sysmon/operational).*\"),\"Endpoint\",\nmatch(search_spl,\".*datamodel(:|=|\\s)(|\\\")Endpoint.*\") OR match(title,\"Threat.*\") OR match(search_spl,\".*sourcetype\\=(|\\\")(wdtap:alerts).*\"),\"Malware\",\nmatch(search_spl,\".*\\`(okta|gws_reports_login)\\`.*\") OR match(search_spl,\".*datamodel(:|=|\\s)(|\\\")Authentication.*\"),\"Authentication\",\nmatch(search_spl,\".*datamodel(:|=|\\s)(|\\\")Change.*\"),\"Change\",\nmatch(search_spl,\".*\\`(stream_http)\\`.*\") OR match(search_spl,\".*datamodel(:|=|\\s)(|\\\")Web.*\"),\"Web\",\nmatch(search_spl,\".*\\`(o365_management_activity|gsuite_gmail)\\`.*\") OR match(search_spl,\".*datamodel(:|=|\\s)(|\\\")Email.*\"),\"Email\",\nmatch(search_spl,\".*\\`(gsuite_gdrive)\\`.*\") OR match(search_spl,\".*datamodel(:|=|\\s)(|\\\")Data Loss.*\"),\"DLP\",\nmatch(search_spl,\".*datamodel(:|=|\\s)(|\\\")Alerts.*\"),\"Alerts\",\nmatch(search_spl,\".*datamodel(:|=|\\s)(|\\\")Intrusion.*\"),\"IDS\",\nmatch(search_spl,\".*\\`(cisco_networks)\\`.*\") OR match(search_spl,\".*datamodel(:|=|\\s)(|\\\")Network.*\"),\"Network\",\nmatch(search_spl,\".*\\`(kubernetes_azure|azuread|cloudtrail|aws_securityhub_finding|aws_cloudwatchlogs_eks|azure_audit|google_gcp_pubsub_message|aws_s3_accesslogs)\\`.*\"),\"Cloud\",\ntrue(),\"Unknown\")\n| fields - search_spl\n| outputlookup RR_sources.csv\n</code></pre> <p>Which looks at the SPL of a search to determine which sourcetype to group it under. Please modify this search as you see fit for your environment. This allows you to create a Risk Incident Rule like this:</p> <pre><code>...\n| eval capped_risk_score=if(summed_risk_score &lt; single_risk_score*2, summed_risk_score, single_risk_score*2)\n| lookup RR_sources.csv title AS source OUTPUTNEW data_sourcetype\n| stats values(*) as * sum(capped_risk_score) as capped_risk_score sum(summed_risk_score) as summed_risk_score dc(annotations.mitre_attack.mitre_tactic_id) as mitre_tactic_id_count dc(annotations.mitre_attack.mitre_technique_id) as mitre_technique_id_count sum(risk_event_count) as risk_event_count dc(source) as source_count values(data_sourcetype) as sourcetypes dc(data_sourcetype) as sourcetype_count\n BY risk_object risk_object_type\n| fields - single_risk_score count\n| eval risk_score = summed_risk_score\n| where sourcetype_count &gt; 1\n</code></pre>"},{"location":"searches/risk_incident_rule_ideas/#events-from-multiple-sourcetypes-with-meta-scoring","title":"Events from Multiple Sourcetypes with Meta-Scoring","text":"<p>Sometimes, you may need more ways of distinguishing which events should have more relevance in an alert beyond a simple count or distinct count. The gist of this strategy is to declare a new variable with a value of 0, then utilize multiple <code>eval</code> statements to add to this value based on attributes about the event. Remember that a <code>case()</code> statement will only apply once and will apply the first match it finds, so you want to ensure your most important matches hit first. Don't be afraid to stack multiple <code>eval</code> statements, and you'll have to tweak what the threshold is depending on the values you chose.</p> <pre><code>...\n| eval capped_risk_score=if(summed_risk_score &lt; single_risk_score*2, summed_risk_score, single_risk_score*2)\n| lookup RR_sources.csv title AS source OUTPUTNEW data_sourcetype\n| rex field=risk_message \"Severity\\=(?&lt;severity&gt;\\w*)\\s\"\n| stats values(*) as * sum(capped_risk_score) as capped_risk_score sum(summed_risk_score) as summed_risk_score dc(annotations.mitre_attack.mitre_tactic_id) as mitre_tactic_id_count dc(annotations.mitre_attack.mitre_technique_id) as mitre_technique_id_count sum(risk_event_count) as risk_event_count dc(source) as source_count values(data_sourcetype) as sourcetypes dc(data_sourcetype) as sourcetype_count\n BY risk_object risk_object_type\n| fields - single_risk_score count\n| eval risk_score = summed_risk_score\n| eval sourcetype_mod = 0\n| eval sourcetype_mod = if(match(sourcetypes,\"Endpoint\"),sourcetype_mod+20,sourcetype_mod)\n| eval sourcetype_mod = if(match(sourcetypes,\"Malware\"),sourcetype_mod+20,sourcetype_mod)\n| eval sourcetype_mod = if(match(sourcetypes,\"Web\"),sourcetype_mod+10,sourcetype_mod)\n| eval sourcetype_mod = if(match(sourcetypes,\"DLP\"),sourcetype_mod+10,sourcetype_mod)\n| eval sourcetype_mod = case(\nmatch(sourcetypes,\"IDS\") AND match(severity,\"(high|critical)\"),sourcetype_mod+20,\nmatch(sourcetypes,\"IDS\"),sourcetype_mod+10,\ntrue(),sourcetype_mod)\n| where sourcetype_mod &gt; 39\n</code></pre> <p>Because <code>sourcetypes</code> is now a multi-valued field by risk_object, I had to create multiple <code>eval</code> checks so that the operation would apply more than once if events from multiple sourcetypes were found. You can also see how I pulled out severity from the risk_message earlier on with <code>rex</code> so I could make a distinction between higher and lower severity IDS events in the meta-scoring. This assumes only my IDS events have that particular formatting to indicate severity; you may have to use more logic to distinguish different sourcetypes and severities, it's just an example. </p> <p>For the scoring threshold of 40, I chose this because of how I've structured the score additions. I will get an alert if a risk object has events from:</p> <ul> <li>2 of (Endpoint / Malware / IDS High-Critical)</li> <li>1 of (Endpoint / Malware) + 2 of (Web / DLP / IDS Low-Medium)</li> </ul> <p>Which may remove a lot of noise from combinations which aren't as likely to be malicious. It is still worthwhile to occasionally review what doesn't pass the threshold to ensure you've crafted a method that surfaces high-fidelity alerts, or are caught with other Risk Incident Rules.</p>"},{"location":"searches/risk_incident_rule_ideas/#mitre-counts-with-meta-scoring","title":"MITRE Counts with Meta-Scoring","text":"<p>The meta-scoring method is useful for getting more value from your MITRE count thresholding rules. </p> <pre><code>...\n| eval capped_risk_score=if(summed_risk_score &lt; single_risk_score*2, summed_risk_score, single_risk_score*2)\n| eval mitre_weight = case(\ncapped_risk_score&gt;70,\"0\",\ncapped_risk_score&gt;40,\"0.5\",\ncapped_risk_score&gt;5,\"0.75\",\ntrue(),\"1\")\n| eval mitre_weight_tactic = mitre_weight * mitre_tactic_id_count\n| eval mitre_weight_technique = mitre_weight * mitre_technique_id_count\n| eventstats sum(mitre_weight_tactic) as mitre_weight_tactic_total sum(mitre_weight_technique) as mitre_weight_technique_total by risk_object risk_object_type source\n| eval mitre_tactic_id_count = mitre_tactic_id_count - mitre_weight_tactic_total\n| eval mitre_technique_id_count = mitre_technique_id_count - mitre_weight_technique_total\n| stats values(*) as * sum(capped_risk_score) as capped_risk_score sum(summed_risk_score) as summed_risk_score sum(mitre_tactic_id_count) as mitre_tactic_id_count sum(mitre_technique_id_count) as mitre_technique_id_count sum(risk_event_count) as risk_event_count dc(source) as source_count\n BY risk_object risk_object_type\n| fields - mitre_weight* single_risk_score count\n| eval risk_score = summed_risk_score\n| eval mitre_mod = 0\n| eval mitre_mod = case(\nmitre_tactic_id_count &gt; 3,mitre_mod+20,\nmitre_tactic_id_count &lt; 4 AND mitre_tactic_id_count &gt; 1,mitre_mod+10,\ntrue(),mitre_mod)\n| eval mitre_mod = case(\nmitre_technique_id_count &gt; 4,mitre_mod+20,\nmitre_technique_id_count &lt; 5 AND mitre_technique_id_count &gt; 2,mitre_mod+10,\ntrue(),mitre_mod)\n| eval mitre_mod = case(\nmvcount(source) &gt; 4,mitre_mod+20,\nmvcount(source) &lt; 5 AND mvcount(source) &gt; 1,mitre_mod+10,\ntrue(),mitre_mod)\n| eval mitre_mod = case(\nmatch(sourcetypes,\"(Malware|Endpoint)\"),mitre_mod+20,\nmatch(sourcetypes,\"IDS\"),mitre_mod+10,\ntrue(),mitre_mod)\n| eval mitre_mod = case(\nmatch(user_category,\"(privileged|technical|executive|watchlist)\"),mitre_mod+20,\nmatch(src_category,\"(Server|DMZ)\"),mitre_mod+10,\ntrue(),mitre_mod)\n| where mitre_mod &gt; 50\n</code></pre> <p>Near the beginning, we juggle some logic for counting events differently which have a lower risk score because when we aggregate on the count of MITRE Tactics/Techniques involved, we might want to treat events with a higher risk score as counting more heavily toward the overall total. This is especially true when aggregating events over longer periods like the out of the box 7 day rule, or something going as far back as 30 or 90 days.</p> <p>Now in the meta-scoring, we have all sorts of ways to distinguish what might be more relevant to us. Now we incorporate:</p> <ul> <li>Number of risk-score-weight-adjusted MITRE tactics</li> <li>Number of risk-score-weight-adjusted MITRE techniques</li> <li>Number of distinct rules firing</li> <li>Rules from particular sourcetypes adding more weight</li> <li>Specific user or system categories adding more weight</li> </ul> <p>Which gives us more control over the types of events that might bubble up in our alerts.</p>"},{"location":"searches/risk_info_event_detail/","title":"Risk info field","text":""},{"location":"searches/risk_info_event_detail/#create-macro-for-risk_info-field","title":"Create macro for risk_info field","text":"<p>You may want to keep risk_message relatively brief as a sort of high-level overview of a risk event, then utilize a new field to store details. We can create a macro called <code>risk_info(1)</code> to create a JSON-formatted field with this SPL:</p> Macro definition<pre><code>eval risk_info = \"{\\\"risk_info\\\":{\"\n| foreach $fields$\n    [\n| eval &lt;&lt;FIELD&gt;&gt;=if(isnull(&lt;&lt;FIELD&gt;&gt;), \"unknown\", &lt;&lt;FIELD&gt;&gt;)\n    ```Preparing json array if FIELD is multivalue, otherwise simple json value```\n| eval json=if(mvcount(&lt;&lt;FIELD&gt;&gt;)&gt;1,mv_to_json_array(mvdedup(&lt;&lt;FIELD&gt;&gt;)),\"\\\"\".&lt;&lt;FIELD&gt;&gt;.\"\\\"\") \n    | eval risk_info=risk_info.\"\\\"\".\"&lt;&lt;FIELD&gt;&gt;\".\"\\\": \".json.\",\"\n    ]\n| rex mode=sed field=risk_info \"s/,$/}}/\"\n| fields - json\n</code></pre> <p>  Many thanks to RedTigR on the RBA Slack for providing the multi-value friendly version of this macro.</p> <p>Utilizing the macro like <code>risk_info(\"field1,field2,field3,etc\")</code> to give us a JSON formatted field with any of the fields we like.</p> <p>And then if we wanted to break this out in a dashboard we could use <code>spath</code> to break out fields into their own columns, or a rex command like this:</p> <p>Example</p> <pre><code>| rex field=risk_info max_match=100 \"(?&lt;risk_info2&gt;\\\"\\w+\\\":\\s*((?:(?&lt;!\\\\\\)\\\"[^\\\"]*\\\"|\\[[^\\]]*\\]))(?=,|\\s*}))\"\n</code></pre> <p>To break out each field as a multi-value on their own line in the same column. It looks really pretty, and you can even use <code>$click.value2$</code> to determine exactly which MV field was clicked and utilize different drilldowns per field, for example.</p>"},{"location":"searches/risk_info_event_detail/#extracting-existing-fields-from-risk-events-into-risk_info-field","title":"Extracting existing fields from risk events into risk_info field","text":"<p>Assumption</p> <p>Your risk rules are outputting specific details in addition to the risk fields (e.g. <code>risk_message</code>, <code>risk_object</code> etc.)</p> <p>The following search replaces the <code>View the individual Risk Attributions</code> drilldown within a risk incident rule. It allows us to dynamically bring the output of each individual risk rule in a concise manner.</p> <p>The aim of this is to minimize pivoting when performing the initial assessment of a risk incident while keeping the notable and <code>risk_message</code> field concise.</p> <pre><code>index=risk\n| search risk_object=$risk_object$\n| rename annotations.mitre_attack.mitre_tactic_id AS mitre_tactic_id, annotations.mitre_attack.mitre_tactic AS mitre_tactic\n| rex field=_raw max_match=0 \"(?&lt;risk_info&gt;[^\\=]+\\=\\\"([^\\\"]+\\\")+?)((, )|$)\"\n| eval risk_info=mvfilter(NOT match(risk_info, \"^(annotations)|(info_)|(savedsearch_description)|(risk_)|(orig_time)|(([0-9]+, )?search_name)\"))\n| table _time, source, risk_object, risk_score, risk_message, risk_info, risk_object_type, mitre_tactic_id, mitre_tactic\n| eval calculated_risk_score=risk_score\n| sort _time\n</code></pre> <p>Breaking down some decisions:</p> <ul> <li><code>| rex field=_raw</code> instead of <code>| foreach *</code> since Splunk adds in additional fields which aren't in the original risk rule output. This was made so the output is as concise and as relevant as possible. However, foreach is another method and it isn't reliant on regex.</li> <li><code>calculated_risk_score</code> is a required field for the drilldown so it displays properly in the Risk Events panel.</li> <li>If you are providing _time in your risk rules, you could rename <code>_time</code> to <code>observation_time</code> and <code>orig_time</code> to <code>_time</code> for a more accurate chronological order of events.</li> <li>The datamodel could be used, but if you wanted accelerated searching via <code>tstats</code> you would need to customize it in some way such as including the <code>_raw</code> field, which may be costly. Creating a risk_info field with the macro above would be more efficient.</li> </ul> <p>Authors</p> @7thdrxn - Haylee Mills @RedTigR @elusive-mesmer"},{"location":"searches/this_then_that_alerts/","title":"Detect Chain of Behaviors","text":"<p>To make a risk rule that looks for two rules firing close together, we can use <code>sort</code> followed by the <code>autoregress</code> command within a certain duration:</p> <pre><code>index=risk sourcetype=stash search_name=\"Search1\" OR search_name=\"Search2\"\n| sort by user _time | dedup _time search_name user\n| delta _time as gap\n| autoregress search_name as prev_search\n| autoregress user as prev_user\n| where user = prev_user\n| table _time gap src user prev_user search_name prev_search\n| where ((search_name=\"Search1\" OR search_name=\"Search2\") AND (prev_search=\"Search1\" OR prev_search=\"Search2\") AND gap&lt;600)\n</code></pre> <p>The benefit of not doing this in a single search is you still have the individual risk events as useful observations, and then can add more risk when observed together, or tweak risk down for noisy events without \"allowlisting\" altogether.</p> <p>Authors</p> @7thdrxn - Haylee Mills"}]}